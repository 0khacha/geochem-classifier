# geochemical_analysis.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class GeochemicalAnalyzer:
    def __init__(self, file_path="dataset/raw/BDD_ICP.xlsx"):
        """
        Initialiser l'analyseur g√©ochimique
        """
        self.df = None
        self.df_clean = None
        self.load_and_prepare_data(file_path)
        
    def load_and_prepare_data(self, file_path):
        """
        Charger et nettoyer les donn√©es
        """
        # Charger le fichier Excel
        self.df = pd.read_excel(file_path)
        
        # Remplacer les virgules par des points
        self.df = self.df.applymap(lambda x: str(x).replace(",", ".") if isinstance(x, str) else x)
        
        # Fonction pour g√©rer "<value"
        def convert_threshold(value):
            if isinstance(value, str) and value.startswith("<"):
                try:
                    return float(value[1:]) / 2
                except:
                    return 0
            try:
                return float(value)
            except:
                return 0
        
        # Conversion des colonnes num√©riques
        for col in self.df.columns:
            if col != "Type":
                self.df[col] = self.df[col].apply(convert_threshold)
                self.df[col] = pd.to_numeric(self.df[col], errors="coerce").fillna(0)
        
        # Encodage de la colonne "Type"
        type_mapping = {"Sterile": 0, "Potentiel": 1, "Minerai": 2}
        self.df["Type"] = self.df["Type"].map(type_mapping)
        
        # Cr√©er version avec noms lisibles pour les graphiques
        self.df_clean = self.df.copy()
        inv_mapping = {0: "Sterile", 1: "Potentiel", 2: "Minerai"}
        self.df_clean["Type_Name"] = self.df_clean["Type"].map(inv_mapping)
        
        print(f"‚úÖ Donn√©es charg√©es: {self.df.shape}")
        print(f"üìä Distribution des types: \n{self.df_clean['Type_Name'].value_counts()}")
    
    def statistical_analysis(self):
        """
        Analyse statistique des diff√©rences entre types
        """
        print("\n" + "="*60)
        print("ANALYSE STATISTIQUE PAR TYPE")
        print("="*60)
        
        # Statistiques descriptives par type
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.drop(['Type'])
        
        stats_by_type = {}
        for type_name in ["Sterile", "Potentiel", "Minerai"]:
            type_code = {"Sterile": 0, "Potentiel": 1, "Minerai": 2}[type_name]
            subset = self.df[self.df["Type"] == type_code]
            stats_by_type[type_name] = subset[numeric_cols].describe()
        
        # Test ANOVA pour chaque √©l√©ment
        print("\nüî¨ Tests ANOVA (p-values < 0.05 = diff√©rences significatives):")
        print("-" * 70)
        
        significant_elements = []
        anova_results = []
        
        for col in numeric_cols:
            sterile_vals = self.df[self.df["Type"] == 0][col]
            potentiel_vals = self.df[self.df["Type"] == 1][col]
            minerai_vals = self.df[self.df["Type"] == 2][col]
            
            # Test ANOVA
            f_stat, p_value = stats.f_oneway(sterile_vals, potentiel_vals, minerai_vals)
            
            significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
            
            anova_results.append({
                'Element': col,
                'F_statistic': f_stat,
                'p_value': p_value,
                'Significance': significance
            })
            
            if p_value < 0.05:
                significant_elements.append(col)
                print(f"{col:15s}: p={p_value:.4f} {significance}")
        
        print(f"\n‚úÖ {len(significant_elements)} √©l√©ments montrent des diff√©rences significatives")
        
        return stats_by_type, anova_results, significant_elements
    
    def feature_importance_analysis(self):
        """
        Analyse d'importance des features avec Random Forest
        """
        print("\n" + "="*60)
        print("IMPORTANCE DES FEATURES (Random Forest)")
        print("="*60)
        
        X = self.df.drop(["Type"], axis=1)
        y = self.df["Type"]
        
        # Entra√Æner Random Forest
        rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced")
        rf.fit(X, y)
        
        # R√©cup√©rer l'importance des features
        importance_df = pd.DataFrame({
            'Element': X.columns,
            'Importance': rf.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        print("Top 15 √©l√©ments les plus importants:")
        print("-" * 40)
        for idx, row in importance_df.head(15).iterrows():
            print(f"{row['Element']:15s}: {row['Importance']:.4f}")
        
        return importance_df
    
    def correlation_analysis(self):
        """
        Analyse des corr√©lations
        """
        print("\n" + "="*60)
        print("ANALYSE DES CORR√âLATIONS")
        print("="*60)
        
        # Matrice de corr√©lation
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        corr_matrix = self.df[numeric_cols].corr()
        
        # Corr√©lations avec le Type
        type_correlations = corr_matrix['Type'].drop('Type').sort_values(key=abs, ascending=False)
        
        print("Corr√©lations les plus fortes avec le Type:")
        print("-" * 45)
        for element, corr in type_correlations.head(15).items():
            direction = "‚Üë" if corr > 0 else "‚Üì"
            print(f"{element:15s}: {corr:6.3f} {direction}")
        
        return corr_matrix, type_correlations
    
    def create_visualizations(self, stats_by_type, importance_df, type_correlations):
        """
        Cr√©er les visualisations
        """
        print("\nCr√©ation des visualisations...")
        
        # Configuration des graphiques
        plt.style.use('default')
        colors = ['#FF6B6B', "#10C662", '#45B7D1']
        
        # Figure 1: Distribution des types
        fig = plt.figure(figsize=(20, 15))
        
        # Subplot 1: Distribution des types
        ax1 = plt.subplot(2, 3, 1)
        type_counts = self.df_clean['Type_Name'].value_counts()
        bars = ax1.bar(type_counts.index, type_counts.values, color=colors)
        ax1.set_title('Distribution des Types de Roches', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Nombre d\'√©chantillons')
        
        # Ajouter les valeurs sur les barres
        for bar, value in zip(bars, type_counts.values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    str(value), ha='center', va='bottom', fontweight='bold')
        
        # Subplot 2: Top 10 Feature Importance
        ax2 = plt.subplot(2, 3, 2)
        top_features = importance_df.head(10)
        bars = ax2.barh(range(len(top_features)), top_features['Importance'], color='skyblue')
        ax2.set_yticks(range(len(top_features)))
        ax2.set_yticklabels(top_features['Element'])
        ax2.set_title('Top 10 - Importance des Features', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Importance')
        ax2.invert_yaxis()
        
        # Subplot 3: Corr√©lations avec Type
        ax3 = plt.subplot(2, 3, 3)
        top_corr = type_correlations.head(10)
        colors_corr = ['red' if x < 0 else 'green' for x in top_corr.values]
        bars = ax3.barh(range(len(top_corr)), top_corr.values, color=colors_corr, alpha=0.7)
        ax3.set_yticks(range(len(top_corr)))
        ax3.set_yticklabels(top_corr.index)
        ax3.set_title('Top 10 - Corr√©lations avec Type', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Corr√©lation')
        ax3.axvline(x=0, color='black', linestyle='--', alpha=0.5)
        ax3.invert_yaxis()
        
        # Subplot 4: Boxplot des √©l√©ments les plus importants
        ax4 = plt.subplot(2, 3, 4)
        top_element = importance_df.iloc[0]['Element']
        self.df_clean.boxplot(column=top_element, by='Type_Name', ax=ax4)
        ax4.set_title(f'Distribution de {top_element} par Type')
        ax4.set_xlabel('Type')
        ax4.set_ylabel(top_element)
        plt.suptitle('')  # Supprimer le titre automatique
        
        # Subplot 5: Scatter plot 2D des 2 √©l√©ments les plus importants
        ax5 = plt.subplot(2, 3, 5)
        elem1 = importance_df.iloc[0]['Element']
        elem2 = importance_df.iloc[1]['Element']
        
        for i, (type_name, color) in enumerate(zip(['Sterile', 'Potentiel', 'Minerai'], colors)):
            mask = self.df_clean['Type_Name'] == type_name
            ax5.scatter(self.df_clean[mask][elem1], self.df_clean[mask][elem2], 
                       c=color, label=type_name, alpha=0.7, s=50)
        
        ax5.set_xlabel(elem1)
        ax5.set_ylabel(elem2)
        ax5.set_title(f'{elem1} vs {elem2}')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # Subplot 6: Heatmap des corr√©lations (√©l√©ments les plus importants)
        ax6 = plt.subplot(2, 3, 6)
        top_elements = importance_df.head(8)['Element'].tolist() + ['Type']
        corr_subset = self.df[top_elements].corr()
        
        im = ax6.imshow(corr_subset.values, cmap='RdBu_r', vmin=-1, vmax=1)
        ax6.set_xticks(range(len(top_elements)))
        ax6.set_yticks(range(len(top_elements)))
        ax6.set_xticklabels(top_elements, rotation=45, ha='right')
        ax6.set_yticklabels(top_elements)
        ax6.set_title('Matrice de Corr√©lation\n(Top Features)')
        
        # Ajouter les valeurs de corr√©lation
        for i in range(len(top_elements)):
            for j in range(len(top_elements)):
                text = ax6.text(j, i, f'{corr_subset.values[i, j]:.2f}',
                               ha="center", va="center", color="white", fontsize=8)
        
        plt.colorbar(im, ax=ax6, shrink=0.8)
        
        plt.tight_layout()
        plt.show()
        
        # Figure 2: Comparaisons d√©taill√©es des top 6 √©l√©ments
        fig2, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        top_6_elements = importance_df.head(6)['Element'].tolist()
        
        for i, element in enumerate(top_6_elements):
            ax = axes[i]
            
            # Boxplot
            data_by_type = [self.df_clean[self.df_clean['Type_Name'] == t][element] 
                           for t in ['Sterile', 'Potentiel', 'Minerai']]
            
            bp = ax.boxplot(data_by_type, labels=['Sterile', 'Potentiel', 'Minerai'],
                           patch_artist=True)
            
            # Colorer les boxplots
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            
            ax.set_title(f'{element}', fontweight='bold')
            ax.grid(True, alpha=0.3)
            
            # Ajouter les moyennes
            for j, type_name in enumerate(['Sterile', 'Potentiel', 'Minerai']):
                mean_val = self.df_clean[self.df_clean['Type_Name'] == type_name][element].mean()
                ax.scatter(j+1, mean_val, color='red', s=50, marker='D', zorder=10)
        
        plt.suptitle('Distribution des 6 √âl√©ments les Plus Discriminants', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    def generate_summary_report(self, stats_by_type, importance_df, type_correlations, significant_elements):
        """
        G√©n√©rer un rapport de synth√®se
        """
        print("\n" + "="*80)
        print("üìã RAPPORT DE SYNTH√àSE - RELATIONS G√âOCHIMIQUES")
        print("="*80)
        
        print(f"\nüéØ √âL√âMENTS DISCRIMINANTS CL√âS:")
        print("-" * 40)
        
        # Top 5 par importance
        top_5_importance = importance_df.head(5)
        print("\n1Ô∏è‚É£ Top 5 par importance (Random Forest):")
        for idx, row in top_5_importance.iterrows():
            print(f"   ‚Ä¢ {row['Element']:15s}: {row['Importance']:.4f}")
        
        # Top 5 par corr√©lation
        top_5_corr = type_correlations.head(5)
        print("\n2Ô∏è‚É£ Top 5 par corr√©lation avec le type:")
        for element, corr in top_5_corr.items():
            trend = "favorise Minerai" if corr > 0 else "favorise Sterile"
            print(f"   ‚Ä¢ {element:15s}: {corr:6.3f} ({trend})")
        
        print(f"\nüß™ ANALYSE STATISTIQUE:")
        print("-" * 40)
        print(f"   ‚Ä¢ {len(significant_elements)} √©l√©ments montrent des diff√©rences significatives (ANOVA p<0.05)")
        print(f"   ‚Ä¢ √âl√©ments les plus discriminants: {', '.join(significant_elements[:5])}")
        
        print(f"\nüíé CARACT√âRISTIQUES PAR TYPE:")
        print("-" * 40)
        
        # Analyser les tendances pour chaque type
        for type_name in ["Sterile", "Potentiel", "Minerai"]:
            type_code = {"Sterile": 0, "Potentiel": 1, "Minerai": 2}[type_name]
            subset = self.df[self.df["Type"] == type_code]
            
            print(f"\n{type_name.upper()}:")
            
            # √âl√©ments enrichis (> moyenne g√©n√©rale)
            enriched = []
            depleted = []
            
            for element in importance_df.head(10)['Element']:
                if element in self.df.columns:
                    type_mean = subset[element].mean()
                    global_mean = self.df[element].mean()
                    
                    if type_mean > global_mean * 1.2:  # 20% au-dessus de la moyenne
                        enriched.append(f"{element} ({type_mean/global_mean:.1f}x)")
                    elif type_mean < global_mean * 0.8:  # 20% en-dessous de la moyenne
                        depleted.append(f"{element} ({type_mean/global_mean:.1f}x)")
            
            if enriched:
                print(f"   ‚ÜóÔ∏è  Enrichi en: {', '.join(enriched[:3])}")
            if depleted:
                print(f"   ‚ÜòÔ∏è  Appauvri en: {', '.join(depleted[:3])}")
        
        print(f"\nüî¨ RECOMMANDATIONS ANALYTIQUES:")
        print("-" * 40)
        print(f"   ‚Ä¢ √âl√©ments prioritaires √† analyser: {', '.join(importance_df.head(5)['Element'])}")
        print(f"   ‚Ä¢ Ratios g√©ochimiques utiles: {top_5_importance.iloc[0]['Element']}/{top_5_importance.iloc[1]['Element']}")
        print(f"   ‚Ä¢ Surveillez particuli√®rement: {', '.join([e for e in significant_elements[:3]])}")
        
        print("\n" + "="*80)
    
    def run_complete_analysis(self):
        """
        Ex√©cuter l'analyse compl√®te
        """
        print("üöÄ D√âMARRAGE DE L'ANALYSE G√âOCHIMIQUE COMPL√àTE")
        print("="*80)
        
        # 1. Analyse statistique
        stats_by_type, anova_results, significant_elements = self.statistical_analysis()
        
        # 2. Analyse d'importance des features
        importance_df = self.feature_importance_analysis()
        
        # 3. Analyse des corr√©lations
        corr_matrix, type_correlations = self.correlation_analysis()
        
        # 4. Visualisations
        self.create_visualizations(stats_by_type, importance_df, type_correlations)
        
        # 5. Rapport de synth√®se
        self.generate_summary_report(stats_by_type, importance_df, type_correlations, significant_elements)
        
        return {
            'stats_by_type': stats_by_type,
            'importance_df': importance_df,
            'type_correlations': type_correlations,
            'significant_elements': significant_elements,
            'anova_results': anova_results
        }

# Utilisation
if __name__ == "__main__":
    # Cr√©er l'analyseur
    analyzer = GeochemicalAnalyzer("dataset/raw/BDD_ICP.xlsx")
    
    # Ex√©cuter l'analyse compl√®te
    results = analyzer.run_complete_analysis()
    
    # Optionnel: Sauvegarder les r√©sultats
    import pickle
    with open("geochemical_analysis_results.pkl", "wb") as f:
        pickle.dump(results, f)
    
    print("\n‚úÖ Analyse termin√©e! R√©sultats sauvegard√©s dans 'geochemical_analysis_results.pkl'")